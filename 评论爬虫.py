from __future__ import print_function
import requests
import json
import re #æ­£åˆ™åŒ¹é…
import time #æ—¶é—´å¤„ç†æ¨¡å—
import jieba #ä¸­æ–‡åˆ†è¯
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as font_manager
from PIL import Image
from wordcloud import WordCloud  #ç»˜åˆ¶è¯äº‘æ¨¡å—
from snownlp import SnowNLP

# import paddlehub as hub


def getMovieinfo(url):
    '''
    è¯·æ±‚çˆ±å¥‡è‰ºè¯„è®ºæ¥å£ï¼Œè¿”å›responseä¿¡æ¯
    å‚æ•°  url: è¯„è®ºçš„url
    return: responseä¿¡æ¯
    '''
    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.113 Safari/537.36",

    }
    response = session.get(url, headers=headers)
    if response.status_code == 200:
         return response.text
    return None

#è§£æjgonæ•°éœ²ï¼Œè·å–è¯„è®º
def saveMoviernforoFile(lastId,arr,url):

    # '''
    # è§£æjsonæ•°æ®ï¼Œè·å–è¯„è®º
    # çˆ¬å–çˆ±å¥‡è‰ºè¯„è®ºåŒº
    # #å‚æ•°lastrdï¼šæœ€åä¸€æ¡è¯„è®ºID arxï¼šå­˜æ•…æ–‡æœ¬çš„1ist
    # ï¼šreturnï¼šæ–°çš„lastrd
    # '''
    if url == None:
        url="https://sns-comment.iqiyi.com/v3/comment/get_comments.action?agent_type=118&agent_version=9.11.5&authcookie=null&business_type=17&content_id=15472234400&hot_size=0&page=&page_size=20&types=time&last_id="

    url+=str(lastId)
    responseTxt=getMovieinfo(url)
    responseJson=json.loads(responseTxt)
    comments=responseJson['data']['comments']
    for val in comments:
        #print(val.keys())
        if'content'in val.keys():
            print(val['content'])
            arr.append(val['content'])
            lastId=str(val['id'])
    return lastId

#è§£æjgonæ•°éœ²ï¼Œè·å–è¯„è®º
def saveMoviernforoFileBili(lastId,arr,url):

    # '''
    # çˆ¬å–bilibiliè¯„è®ºåŒº
    # è§£æjsonæ•°æ®ï¼Œè·å–è¯„è®º
    # #å‚æ•°lastrdï¼šæœ€åä¸€æ¡è¯„è®ºID arxï¼šå­˜æ•…æ–‡æœ¬çš„1ist
    # ï¼šreturnï¼šæ–°çš„lastrd
    # 752881391å›å½¢é’ˆ
    # 582921863
    # '''
    if url == None:
        url="https://api.bilibili.com/x/v2/reply?callback=jQuery17202354826870397837_1588331328657&type=1&oid=752881391&sort=2&_=1588331336930&pn="
    url+=str(lastId)
    responseTxt=getMovieinfo(url)
    responseJson=json.loads(responseTxt)
    comments=responseJson['data']['replies']
    for val in comments:
        #print(val.keys())
        if'content'in val.keys():
            print(val['content']['message'])
            arr.append(val['content']['message'])
    return lastId


def clear_special_char(content):

    #æ¸…é™¤æ²¡æœ‰æ„ä¹‰çš„å­—ç¬¦ï¼Œæ•°æ®æ¸…æ´—

    s = re.sub(r"</?(.+?)>|&nbsp;|\t|\r", "", content)
    s = re.sub(r"\n", "", s)
    s = re.sub(r"\*", "\\*", s)
    s = re.sub("\u4e00-\u9fa5^a-z^A-Z^0-9", "", s)
    s = re.sub(
        '[\001\002\003\004\005\006\007\x08\x09\x0a\x0b\x0c\x0d\x0e\x0f\x10\x11\x12\x13\x14\x15\x16\x17\x18\x19\x1a]+',
        '', s)
    s = re.sub('[a-zA-Z]', "", s)
    s = re.sub('^\d+(\.\d+)?$', "", s)
    s = re.sub(u'[\U00010000-\U0010ffff]', ' ', s)
    s = re.sub(u'[\uD800-\uDBFF][\uDC00-\uDFFF]', ' ', s)
    for ch in 'ï¼Œã€‚ï¼›ï¼šâ€œâ€ã€ã€‹ã€Šã€|*&â€¦ğŸ™ï¼â™¥â™¡ğŸ˜ŠğŸŒšï¼ŸğŸ’šâˆšğŸ¼ã€ã€‘ğŸ’”ğŸ´]à¹‘ğŸ‘[ğŸŒŸğŸ˜˜ğŸ¤˜ï¾‰ğŸ±ğŸ‘©â€â¤â€œğŸ’ğŸŒ¸ğŸ’™ğŸ˜â„ï¼Œâ‰§â–½â‰¦ğŸ‘€ğŸ¶ğŸ¬ğŸ˜‚ !ğŸ§¡ğŸ˜ƒ ãƒ¾â†—~â†–ï¼¾ ğŸ»ğŸ‹ï½â™€Ù©à¯°^ ÙˆËƒÍˆ Ì¶Ï‰Ë‚ğŸ˜†à¯° Ë‚ğŸ”’ğŸ§ğŸ’›ğŸ’šğŸ’–Å âˆ€Åâˆ€âœ”ğŸ¤ ( à¸‡ _ â€¢ ã€‚Ì ) à¸‡ğŸ”’âœ¨ğŸ‘ğŸ’™ğŸ’œğŸ‘§ğŸ›ğŸŸâœŠğŸŒ ğŸŒ¨ğŸ’ªâ­â€â€¦':
        s = s.replace(ch, ' ')
    return s





def fenci(text):
    '''
    åˆ©ç”¨jiebaè¿›è¡Œåˆ†è¯
    å‚æ•° text:éœ€è¦åˆ†è¯çš„å¥å­æˆ–æ–‡æœ¬
    returnï¼šåˆ†è¯ç»“æœ
    '''


    jieba.load_userdict('words_bilibili.txt')
    seg = jieba.lcut(text,cut_all = False)
    print(seg)
    return seg


def stopwordslist(file_path):
    '''
    åˆ›å»ºåœç”¨è¯è¡¨
    å‚æ•° file_path:åœç”¨è¯æ–‡æœ¬è·¯å¾„
    returnï¼šåœç”¨è¯list
    '''
    stopwords = [line.strip() for line in open(file_path,encoding='UTF-8').readlines()]


    return stopwords


def movestopwords(sentence,stopwords,counts):
    '''
    å»é™¤åœç”¨è¯,ç»Ÿè®¡è¯é¢‘
    å‚æ•° file_path:åœç”¨è¯æ–‡æœ¬è·¯å¾„ stopwords:åœç”¨è¯list counts: è¯é¢‘ç»Ÿè®¡ç»“æœ
    returnï¼šNone
    '''
    out = []
    for word in sentence:
        if word not in stopwords:
            if len(word)!=1:
                counts[word] = counts.get(word,0)+1

    return None

def drawcounts(counts,num):
    '''
    æ ¹æ®è¯é¢‘ç»˜åˆ¶ç»Ÿè®¡è¡¨

    returnï¼šnone
    '''
    x_aixs = []
    y_aixs = []
    c_order = sorted(counts.items(),key = lambda x:x[1],reverse = True) # å¯¹äºŒç»´æ•°æ®è¿›è¡Œæ’åº
    # print(x_crder)
    for c in c_order[:num]:
        x_aixs.append(c[0])
        y_aixs.append(c[1])

    matplotlib.rcParams['font.sans-serif'] = ['SimHei'] #æŒ‡å®šé»˜è®¤å­—ä½“
    matplotlib.rcParams['axes.unicode_minus'] = False
    plt.bar(x_aixs,y_aixs)
    plt.title('è¯é¢‘ç»Ÿè®¡ç»“æœ')
    plt.show()

def drawcloud(word_f):
    '''
    æ ¹æ®è¯é¢‘ç»˜åˆ¶è¯äº‘å›¾
    å‚æ•° word_f:ç»Ÿè®¡å‡ºçš„è¯é¢‘ç»“æœ
    returnï¼šnone
    '''

    #èƒŒæ™¯å›¾ç‰‡
    # coud_mask = np.array(Image.open(''))
    #å¿½ç•¥æ˜¾ç¤ºçš„è¯
    st = set(["ä¸œè¥¿","è¿™æ˜¯","çœŸçš„"])
    #ç”Ÿæˆ
    wc = WordCloud(background_color='white',
                   mask=None,
                   max_words=150,
                   font_path='simhei.ttf',
                   min_font_size=10,
                   max_font_size=100,
                   width=400,
                   relative_scaling=0.3,
                   stopwords=st
                   )
    wc.fit_words(word_f)
    wc.to_file('pic.png')




def crawler(num,choose,url):
    lastId = '0'
    arr = [] # è¯„è®ºçš„å­˜æ”¾æ•°ç»„
    if choose=="aqy":
        with open('aqy.txt','a',encoding='utf-8') as f:
            for i in range(num):
                lastId = saveMoviernforoFile(lastId,arr,url)
                time.sleep(0.5)
            for item in arr:
                item = clear_special_char(item)  # æ¸…æ¥šç‰¹æ®Šå­—ç¬¦
                if item.strip()!='':
                    try:
                        f.write(item+'\n')
                    except Exception as e:
                        print("å«æœ‰ç‰¹æ®Šå­—ç¬¦")
    elif choose=="bilibili":
        with open('bilibili.txt','a',encoding='utf-8') as f:
            for i in range(num):
                lastId = saveMoviernforoFileBili(i+1,arr,url)
                time.sleep(0.5)
            for item in arr:
                item = clear_special_char(item)  # æ¸…æ¥šç‰¹æ®Šå­—ç¬¦
                if item.strip()!='':
                    try:
                        f.write(item+'\n')
                    except Exception as e:
                        print("å«æœ‰ç‰¹æ®Šå­—ç¬¦")
    else:
        print("æ²¡æœ‰æ­¤é€‰é¡¹")


    print('ä¸€å…±çˆ¬å–ï¼š',len(arr))

def analys(counts,choose):
    if choose =="aqy":
        f = open('aqy.txt', 'r', encoding='utf-8')
        for line in f:
            words = fenci(line)
            stopwords = stopwordslist('add_words.txt')
            movestopwords(words, stopwords, counts)
        f.close()
        np.save('my_file.npy', counts)
    if choose =="bili":
        f = open('bilibili.txt', 'r', encoding='utf-8')
        for line in f:
            words = fenci(line)
            stopwords = stopwordslist('add_words.txt')
            movestopwords(words, stopwords, counts)
        f.close()
        np.save('my_file_bili.npy', counts)



if __name__ == "__main__":
    num = 10 # numæ˜¯é¡µæ•°ï¼Œä¸€é¡µåæ¡è¯„è®ºï¼Œå‡å¦‚çˆ¬å–1000æ¡ï¼Œè®¾ç½®num=100
    counts = {}
    url = None  # å¯æ¢æˆæƒ³è¦çˆ¬å–çš„ç½‘é¡µ
    crawler(num,"bilibili",url) # è¿è¡Œçˆ¬è™«ï¼Œç”Ÿæˆæ–‡ä»¶txt.(num,choose,url)

    analys(counts,"bili") # è¯é¢‘ç»Ÿè®¡è¾“å…¥txtè¾“å‡ºåˆ†è¯npy
    read_dictionary = np.load('my_file_bili.npy',allow_pickle=True).item() # è¯»å–è¯é¢‘ç»Ÿè®¡

    plt.figure(figsize=(50, 6))
    drawcounts(read_dictionary,50)
    drawcloud(read_dictionary)


# ---------------æƒ…æ„Ÿåˆ†ææ¨¡å—---------------------
# åˆ¤æ–­å¥å­çš„æƒ…æ„Ÿæ˜¯ç§¯æè¿˜æ˜¯æ¶ˆæçš„ï¼Œå‡†ç¡®ç‡éå¸¸å·®ä¸å»ºè®®ä½¿ç”¨ã€‚

    # f = open('bilibili.txt', 'r', encoding='utf-8')
    # positive = 0
    # negative = 0
    # for line in f:
    #     a = SnowNLP(line).sentiments
    #     print(line+" "+str(a))
    #     if a >0.6:
    #         positive=positive+1
    #     if a <0.4:
    #         negative=negative+1
    #
    # print(positive)
    # print(negative)

# -----------------------------------------------------


    # çˆ±å¥‡è‰ºæ¨¡å—
    # f = open('aqy.txt','r',encoding='utf-8')
    # for line in f:
    #     words = fenci(line)
    #     stopwords = stopwordslist('add_words.txt')
    #     movestopwords(words,stopwords,counts)
    #
    # drawcounts(read_dictionary,10)
    # drawcloud(read_dictionary)
    # f.close()
